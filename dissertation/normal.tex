\chapter{Functional Verification of Sequential Normal Basis Multiplier}
\label{ch:normal}
In order to utilize our traversal algorithm, it is necessary to find out
a sort of suitable circuit benchmarks which is easy to compute its
Gr\"obner basis (GB). From the work of Lv et al. \cite{lv_dissertation},
we learn that arithmetic circuits in Galois field (GF) is
convertible to an ideal of circuit polynomials, and the 
ideal generators form a GB themselves when applying reverse topological
term order. Furthermore, according to the work of Pruss et al.
\cite{tim_dissertation}, with a limited computation complexity,
we can abstract the word-level signature of an arithmetic 
component working in GF. Thus, we consider the possibility 
of applying our traversal algorithm on sequential Galois
field circuits. In each frame, we can use the techniques 
from \cite{tim_dissertation} to abstract the word-level
signature of the combinational logic, which corresponds
to the transition function in our traversal algorithm.
As a result, we manage to find a type of sequential GF multiplier
which we can apply our traversal algorithm to actually 
verify its functional correctness.

\section{Motivation}
\label{sec:normal_motiv}
From the preliminaries (Chapter \ref{ch:prelim}) about FSMs, we learn that the
Moore machine does not rely on inputs for state transitions. 
As depicted in Figure \ref{fig:Moore}(a), a typical Moore machine implementation
consists of combinational logic component and register files, where
$r_0,\dots,r_k$ are present state (PS) variables 
standing for state inputs (SI), and $r_0',\dots,r_k'$ are next state (NS) variables standing for
state outputs (SO). Figure \ref{fig:Moore}(b) shows the state transition graph (STG) of 
a Moore machine with $k+1$ distinct states. We notice that it forms a simple chain,
with $k$ consecutive transitions the machine reaches final state $R_k$.

\begin{figure}[H]
\centering{
\includegraphics[width=5in]{newfig/Moore.eps}
\caption{A typical Moore machine and its state transition graph}
\label{fig:Moore}}
\end{figure}

In practice,
some arithmetic components are designed in sequential circuits similar to the structure in 
Figure \ref{fig:Moore}(a). Initially the operands are loaded into the registers, 
then the closed circuit executes without taking any additional information from outside,
and store the results in registers after $k$ clock cycles. Its behavior can be described using
STG in Figure \ref{fig:Moore}(b): state $R$ denotes the bits stored in registers. Concretely, $R_init$ is the initial
state (usually reset to all zeros), $R_1$ to $R_{k-1}$ are intermediate results stored as SO of current state and SI
for next state, and $R_k$ (or $R_{final}$) is the final result given by arithmetic circuits (and equals to the answer
to arithmetic function when circuit is working functional correctly).
This kind of design results in 
reusing a smaller combinational logic component such that the area cost is greatly optimized.
However, it also brings difficulties in verifying the the circuit functions.

\begin{figure}[H]
\centering{
\includegraphics[width=5in]{newfig/convention.eps}
\caption{Conventional verification techniques based on bit-level unrolling and equivalence checking}
\label{fig:convention}}
\end{figure}

Conventional methods to such a sequential circuit may consist of unrolling the circuit for 
$k$ time-frames, and performing an equivalence checking between the unrolled machine and
the specification function. However, the number of gates will grow fast when doing unrolling
on bit-level. Meanwhile the structural similarity based equivalence checking techniques 
will fail when the sequential circuit is highly customized and optimized from the naive specification 
function. As a result, conventional techniques is grossly inefficient for large circuits.
Therefore, a new method based on our proposed word-level FSM traversal technique is worthy to be explored.

\section{Normal Basis Multiplier over Galois Field}
From algebraic view, a field is a space, and field elements are dots in the space. Those elements can be 
represented with unique coordinates, which requires the pre-definition of a basis vector. In this
section, we discuss a special basis called normal basis, as well as the advantages adopting it in 
GF operations esp. multiplication.
\subsection{Normal Basis}
Given a Galois field (GF) $\Fkk$ is a finite field with  $2^k$ elements and characteristic equals to 2.
Its elements can be written in polynomials of $\alpha$, when there is an irreducible polynomial $p(\alpha)$
defined.

If we use a basis $\{1,\alpha,\alpha^2,\alpha^3,\dots,\alpha^{k-1}\}$, we can easily transform polynomial representations
to binary bit-vector representations by recording the coefficients. For example,

\begin{table}[H]
\centering
\caption{Bit-vector, Exponential and Polynomial representation of
elements in  ${\mathbb{F}}_{2^4} = {\mathbb{F}}_2[x]
\pmod{x^4+x^3+1}$}
\begin{tabular}{|c|c||c|c|} 
\hline
$a_3a_2a_1a_0$ & Polynomial     &$a_3a_2a_1a_0$ & Polynomial  \\
\hline
$0000$        & $0$           & $1000$  &$\alpha^3$\\
\hline
$0001$        & $1$           & $1001$  & $\alpha^3 + 1$\\
\hline
$0010$        &  $\alpha$       & $1010$ & $\alpha^3 + \alpha$  \\
\hline
$0011$        &  $\alpha + 1$   & $1011$ &  $\alpha^3+\alpha+1$\\
\hline
$0100$        &  $\alpha^2$     &  $1100$ &  $\alpha^3 + \alpha^2$\\
\hline
$0101$        & $\alpha^2 + 1$ & $1101$  & $\alpha^3+\alpha^2+1$\\
\hline
$0110$        &  $\alpha^2 + \alpha$ & $1110$ &  $\alpha^3+\alpha^2+\alpha$\\
\hline
$0111$        & $\alpha^2+\alpha+1$ & $1111$ & $\alpha^3+\alpha^2+\alpha+1$\\
\hline
\end{tabular}
\label{table:booltogalois}  
\end{table}

Basis $\{1,\alpha,\alpha^2,\alpha^3,\dots,\alpha^{k-1}\}$ is called {\bf standard basis} (StdB), which results in
a straightforward representation for elements, and operations of elements such as addition and subtraction.
The addition/subtraction of GF elements in StdB follows the rules of polynomial addition/subtraction
where coefficients belong to $\mathbb F_2$. In other words, using the definition of {\it exclusive or} in
Boolean algebra, element $A$ add/subtract by element $B$ in StdB is defined as
\begin{align}\label{eqn:StdB}
A+B = A-B &= (a_0,a_1,\dots,a_{k-1})_{StdB} \xor (b_0,b_1,\dots,b_{k-1})_{StdB} \nonumber\\
&=(a_0\oplus b_0, a_1\oplus b_1,\dots,a_{k-1}\oplus b_{k-1})_{StdB} 
\end{align}

\subsection{Multiplication using Normal Basis}
Besides addition/subtraction, multiplication is also very common in arithmetic circuit design.
The multiplication of GF elements in $\Fkk$ in StdB follows the rule of polynomial multiplication.
However, it will result in $O(k^2)$ bitwise operations. In other words, if we implement GF multiplication
in bit-level logic circuit, it will contain $O(k^2)$ gates. When the datapath size $k$ is large,
the area and delay of circuit will be costly.

In order to lower down the complexity of arithmetic circuit design, Massey and Omura \cite{MasseyOmura} % ref 7 in RH paper
use a new basis to represent GF elements, which is called {\bf normal basis} (NB).
A normal basis over $\Fkk$ is written in the form of
\begin{equation*}
N.B. ~~~ \N = \{\beta,\beta^2,\beta^4,\beta^8,\dots,\beta^{2^{k-1}}\}
\end{equation*}
Respectively, a field element in NB representation is actually
\begin{align*}
A &= (a_0,a_1,\dots,a_{k-1})_{NB} \\
  &= a_0\beta+a_1\beta^2+\cdots+a_{k-1}\beta^{2^{k-1}} \\
  &= \sum_{i=0}^{k-1} a_i\beta^{2^i}
\end{align*}

According to the definition, a normal basis is a vector where the next entry is the square of the former one.
We note that the vector is cyclic, i.e. $\beta^{2^k} = \beta$ due to {\it Fermat's little theorem}.
{\bf Normal element} $\beta$ is an element from the field which is used to construct the normal basis,
and can be represent as a power of primitive element $\alpha$: 
\begin{equation*}
\beta = \alpha^t, ~~~ 1\leq t<2^k
\end{equation*}

The addition and subtraction of elements in NB representation are similar to equation \ref{eqn:StdB}.
However, what makes NB powerful is its property when doing multiplications and exponentiations.
The following lemmas and examples illustrate this fabulous property very well.
\begin{Lemma}[Square of NB]
\label{lem:squareNB}
In $\Fkk$, equation 
\begin{equation*}
(a+b)^2 = a^2 + b^2
\end{equation*}
has been proved. According to the \textbf{binomial theorem}, it can be extended to
\begin{align*}
&(b_0\beta + b_1\beta^2 + b_2\beta^4 + \dots + b_{k-1}\beta^{2^{k-1}})^2 \\
&= b_0^2\beta^2 + b_1^2\beta^4 + b_2^2\beta^8 + \dots + b_{k-1}^2\beta \\
&= b_{k-1}^2\beta + b_0\beta^2 + b_1\beta^4 + \dots + b_{k-2}\beta^{2^{k-1}}
\end{align*}
\end{Lemma}
This lemma concludes that the square of an element in NB equals to a simple right-cyclic shift of the bit-vector.
Obviously, StdB representation does not have this benefit.

\begin{Example}[Square of NB]
In GF $\mathbb F_{2^3}$ constructed by irreducible polynomial $x^3 + x + 1$, the standard basis is denoted as 
$\{ 1, \alpha, \alpha^2\}$ where $\alpha^3+\alpha+1=0$.
Let $\beta = \alpha^3$, then $\N = \{ \beta, \beta^2, \beta^4\}$ forms a normal basis. 
Write down element $E$ using both representations:
\begin{align*}
E &= (a_0,a_1,a_2)_{StdB} = (b_0,b_1,b_2)_{NB} \\
  &= a_0 + a_1\alpha + a_2\alpha^2 = b_0\beta + b_1\beta^2 + b_2\beta^4
\end{align*}
Compute the square of $E$ in StdB first:
\begin{align*}
E^2 &= a_0 + a_1\alpha^2 + a_2\alpha^4 \\
    &= a_0 + a_2\alpha + (a_1 + a_2)\alpha^2 \\
    &= (a_0,a_2,a_1+a_2)_{StdB}
\end{align*}
When it is computed in NB, we can make it very simple:
\begin{align*}
E^2 &= \overset{\xrightarrow{Cyclic~~shift}}{(b_0,b_1,b_2)}_{NB} \\
	&= (b_2,b_0,b_1)_{NB}
\end{align*}
\end{Example}

This example shows that convenience to use NB when computing $2^k$ power of an element.
Multiplication is a bit complicated than squaring; but when it is decomposed as bit-wise
operations, the property in lemma \ref{lem:squareNB} can be well utilized.

\begin{Example}[Bit-wise NB multiplication]
Assume there are 2 binary vectors representing 2 operands in NB over $\Fkk$: 
$A = (a_0, a_1, \dots, a_{k-1}), B = (b_0, b_1, \dots, b_{k-1})$. Note that in this example, 
by default we use normal basis representation so subscript ``NB" is skipped. Their product can also be written
as: $$C = A\times B = (c_0, c_1, \dots, c_{k-1})$$

Assume the most significant bit (MSB) of the product can be represented by a function $f_{mult}$: 
\begin{equation}
\label{eqn:multiMSB}
c_{n-1} = f_{mult}(a_0, a_1, \dots, a_{n-1}; b_0, b_1, \dots, b_{n-1})
\end{equation}
Before discussing the details of the function $f_{mult}$, we can take 
a square on both side of equation \ref{eqn:multiMSB}, i.e. $C^2 = A^2\times B^2$.
Obviously, using the property in lemma \ref{lem:squareNB}, the original second most significant bit 
becomes the new MSB because of right-cyclic shifting. 
Concretely, 
$$(c_{k-1},c_0,c_1,\dots,c_{k-2}) = (a_{k-1},a_0,a_1,\dots,a_{k-2})\times(b_{k-1},b_0,b_1,\dots,b_{k-2})$$
Note $A^2, B^2$ and $C^2$ still belong to $\Fkk$, thus as a universal function implementing MSB multiplication
over $\Fkk$, $f_{mult}$ still keeps the same. As a result, the new MSB can be written as 
\begin{equation}
\label{eqn:shiftMSB}
c_{k-2} = f_{mult}(a_{k-1}, a_0, a_1, 
\dots, a_{k-2}; b_{k-1}, b_0, b_1, \dots, b_{k-2})
\end{equation}
Similarly, if we take a square again on the new equation, we can get $c_{k-3}$.
Successively we can derive all bits of product $C$ using the same function $f_{mult}$, and the only
adjustment we need to make is to right-cyclic shift 2 operands by 1 bit each time.
\end{Example}

From above example, it is proved that a universal structure that implements $f_{mult}$ can be reused
for $k$ times in NB multiplication over $\Fkk$. Comparing to StdB, which requires distinct design 
for every bit of multiplication, NB is less costly if we can prove $f_{mult}$ will not result in 
a structure with $O(k^2)$ complexity. So our next mission is to explore the details of $f_{mult}$
to prove it will be a relatively simple design with complexity lower than $O(k^2)$.

If we want to make the complexity of $f_{mult}$ lower than $O(k^2)$, then the best choice is to try out 
linear functions. As we know, matrix multiplication can simulate all possible combinations of 
linear functions (which is also the reason it is used as basic model to simulate the behavior of a neuron
in neural network machine learning algorithms). Imagine $A$ is a $k$-bit row vector and $B$ is a $k$-bit 
column vector, then the single bit product can be written as the product of matrix multiplication
$$c_{l} = A\times C\times B$$
where $C$ is a $k\times k$ square matrix.

\begin{Definition}[$\lambda$-Matrix]
A binary $k\times k$ matrix $M$ is used to describe the bit-wise normal basis multiplication function $f_{mult}$ where
\begin{equation}
\label{eqn:def_lambda}
c_{l} = f_{mult}(A, B) = A \times M \times B^T
\end{equation}
$B^T$ denotes vector transposition. Matrix $M$ is called $\lambda$-Matrix of
$k$-bit NB multiplication over $\Fkk$.
\end{Definition} 

When taking different bits $l$ of the product in equation \ref{eqn:def_lambda}, 
we obtain a series of conjugate matrices of $M$. 
Which means instead of shifting operands $A$ and $B$, we can also shift the 
matrix.

More specifically, we denote the matrix by \emph{$l$-th $\lambda$-Matrix} as 
$$c_l = A \times M^{(l)} \cdot B^T$$
Meanwhile, the operator shifting rule in equation \ref{eqn:shiftMSB} still holds. Then we have relation 
$$c_{l-1} = A \cdot M^{(l-1)} \cdot B^T = shift(A) \cdot M^{(l)} \cdot shift(B)^T$$ which means
by right and down cyclically shifting $M^{(l-1)}$, we can get $M^{(l)}$.

\begin{Example}[NB multiplication using $\lambda$-Matrix]
Over GF $\mathbb F_{2^3}$ constructed by irreducible polynomial $\alpha^3 + \alpha + 1$, let normal element $\beta = \alpha^3$, $N = \{ \beta, \beta^2, \beta^4\}$ 
forms a normal basis. Corresponding $0$-th $\lambda$-Matrix is
\begin{equation*}
M^{(0)} = \left(
\begin{array} {lcr}
0 & 1 & 0\\
1 & 0 & 1\\
0 & 1 & 1
\end{array} \right).
\end{equation*}
i.e.,
\begin{equation*}
c_0 = (a_0\  a_1\  a_2)\left(
\begin{array} {lcr}
0 & 1 & 0\\
1 & 0 & 1\\
0 & 1 & 1
\end{array} \right)\left(
\begin{array} {lcr}
b_0\\
b_1\\
b_2
\end{array} \right)
\end{equation*}
From $0$-th $\lambda$-Matrix we can directly write down all remaining $\lambda$-Matrices:
\begin{equation*}
M^{(1)} = \left(
\begin{array} {lcr}
1 & 0 & 1\\
0 & 0 & 1\\
1 & 1 & 0
\end{array} \right)~~~~~~
M^{(2)} = \left(
\begin{array} {lcr}
0 & 1 & 1\\
1 & 1 & 0\\
1 & 0 & 0
\end{array} \right)
\end{equation*}
\end{Example}

If we generalize the definition and explore the nature of $\lambda$-Matrix, it is defined as cross-product terms from multiplication, which is 
\begin{equation}
Product~vector~C = (\sum_{i=0}^{k-1}a_i\beta^{2^i})(\sum_{j=0}^{k-1}b_j\beta^{2^j}) = \sum_{i=0}^{k-1}\sum_{j=0}^{k-1}a_ib_j\beta^{2^i}\beta^{2^j}
\end{equation}
The expressions $\beta^{2^i}\beta^{2^j}$ are referred to as cross-product terms, and can be represented by
NB, i.e.
\begin{equation}
\beta^{2^i}\beta^{2^j} = \sum_{l=0}^{k-1}\lambda_{ij}^{(l)}\beta^{2^l}, \ \ \lambda_{ij}^{(l)} \in \mathbb F_2.
\end{equation}
Substitution yields, result is an expression for l-th digit of product as showed in equation \ref{eqn:multiMSB}:
\begin{equation}
c_l = \sum_{i=0}^{k-1}\sum_{j=0}^{k-1}\lambda_{ij}^{(l)}a_ib_j
\end{equation}
$\lambda_{ij}^{(l)}$ is the entry with coordinate $(i,j)$ in $l$-th $\lambda$-Matrix.

The $\lambda$-Matrix can be implemented with XOR and AND gates in circuit design.
The very naive implementation requires $O(C_N)$ gates, where $C_N$ is the number of
nonzero entries in $\lambda$-Matrix.
There usually exists multiple NBs in $\Fkk, k>3$. If we employ a random NB, there is no mathematical
guarantee that $C_N \sim o(k)$ (symbol $o$ denotes ``strictly lower than bound"). However,
Mullin et al. \cite{mullinONB} % This citation is valid
proves that in certain GF $\mathbb F_{p^{k_{opt}}}$, there always exists at least one NB such that 
its corresponding $\lambda$-Matrix has $C_N = 2n-1$ nonzero entries. A basis with this property is
called optimal normal basis (ONB), details are introduced in appendix.

In practice, large size NB multipliers are usually designed in $\Fkk$ when ONB exists
to minimized the number of gates. So in the following part of this chapter and our experiments,
we only focus on ONB multipliers instead of general NB multipliers.


% After fixing this, add a whole-piece StdB vs NB example, list their cost as the conclusion
\subsection{Comparison between Standard Basis and Normal Basis}
At the end of this section, a detailed example is used to make a comparison between StdB multiplication
and NB multiplication.
\begin{Example}[Rijndael's finite field]
Rijndael uses a characteristic 2 finite field with 256 elements, which can also be called the GF $\mathbb F_{2^8}$.
Let us define the primitive element $\alpha$ using irreducible polynomial 
$\alpha^8+\alpha^7+\alpha^6+\alpha^4+\alpha^2+\alpha+1$. Coincidently, $\alpha$ is also a normal element,
i.e. $\beta = \alpha$ can construct a NB $\{\alpha,\alpha^2,\alpha^4,\alpha^8,\alpha^{16},\alpha^{32},\alpha^{64},
\alpha^{128}\}$.

We pick a pair of elements from the Rijndael's field: $A=(0100~1011)_{StdB} = (4B)_{StdB},~B=(1100~1010)_{StdB} = (CA)_{StdB}$.
First let us compute their product in StdB, the rule follows ordinary polynomial multiplication.

\begin{align*}
A\cdot B &= (\alpha^6+\alpha^3+\alpha+1)(\alpha^7+\alpha^6+\alpha^3+\alpha)\\
&= (\alpha^{13}+\alpha^{10}+\alpha^8+\alpha^7)+(\alpha^{12}+\alpha^9+\alpha^7+\alpha^6)+
	(\alpha^9+\alpha^6+\alpha^4+\alpha^3)\\
	&~~~+(\alpha^7+\alpha^4+\alpha^2+\alpha) \\
&= \alpha^{13}+\alpha^{12}+\alpha^{10}+\alpha^8+\alpha^7+\alpha^3+\alpha^2+\alpha
\end{align*}
Note that this polynomial is not the final form of the product because it needs to be
reduced modulo irreducible polynomial $\alpha^8+\alpha^7+\alpha^6+\alpha^4+\alpha^2+\alpha+1$.
This can be done using base-2 long division. Note the dividend and divisor are written in pseudo Boolean
vectors, not real Boolean vectors in any kind of bases.

% The folowing part is customized latex code mimicing binary long division
% Note \divrule coordinates may not reflect the intuitive column # in tabular. Adjust by urself!!!

\vspace{1cm}

\newdimen\digitwidth
\settowidth\digitwidth{0}
\def~{\hspace{\digitwidth}}


\def\divrule#1#2{%
\noalign{\moveright#1\digitwidth%
\vbox{\hrule width#2\digitwidth}}}
111010111\,\begin{tabular}[b]{@{}r@{}}
101001 \\ \hline
\big)\begin{tabular}[t]{@{}l@{}}
11010110001110 \\
111010111 \\ \divrule{0}{14}
~~111101101 \\
~~111010111 \\ \divrule{2}{12}
~~~~~111010110 \\
~~~~~111010111 \\ \divrule{5}{9}
~~~~~~~~~~~~~1
\end{tabular}
\end{tabular}
\vspace{0.5cm}

The final remainder is $1$, i.e. the product equals to 1 in StdB.

On the other hand, operands $A$ and $B$ can be written in NB as
$$A = (0010~1001)_{NB},~~B = (0100~0010)_{NB}$$
The $\lambda$-Matrix for $\mathbb F_{2}[x] \pmod{x^8+x^7+x^6+x^4+x^2+x+1}$
is
(Computation of $\lambda$-Matrix refers to appendix)
\begin{equation*}
M^{(0)} = \left(\begin{array}{lccccccr}
0 &0 &0 &0 &1 &0 &1 &1 \\
0 &0 &1 &1 &1 &1 &0 &0 \\
0 &1 &0 &0 &0 &0 &1 &0 \\
0 &1 &0 &0 &1 &1 &0 &1 \\
1 &1 &0 &1 &0 &1 &0 &0 \\
0 &1 &0 &1 &1 &0 &0 &1 \\
1 &0 &1 &0 &0 &0 &0 &0 \\
1 &0 &0 &1 &0 &1 &0 &1
\end{array}\right)
\end{equation*}
Taking matrix multiplication $c_0 = A\times M^{(0)}\times B^T$,
the result is $c_0 = 1$. Then by cyclic shifting $A$ and $B$
(or shifting $M^{(0)}$, either is applicable), we can successively
obtain other bits of product. The final answer is
$$C = (0000~0001)_{NB}$$
It is equivalent to the result in StdB.
\end{Example}

From the intuition of humans, StdB multiplication is straightforward and easier to understand
while NB is difficult to comprehend. However, if we implement both multiplications to 
hardware multipliers, it will be clear which side a circuit designer prefers.

% May consider providing details?

Mastrovito multiplier and Montgomery multiplier are 2 common designs
of GF multipliers using StdB. As a naive implementation of GF multiplication,
Mastrovito multiplier uses most number of gates:
$k^2$ AND gates plus $k^2-\Delta$ XOR gates \cite{Mastrovito}. Montgomery multiplier 
applies lazy reduction techniques and results in a better latency performance, while the number of gates are about
the same with Mastrovito multiplier:
$k^2$ AND gates plus $k^2-k/2$ XOR gates \cite{Montgomery}. 
Concretely, typical design of Mastrovito multiplier consists of 218 logic gates, while 
Montgomery multiplier needs 198 gates. However, the NB multiplier reuses the $\lambda$-Matrix 
logic, so this component will only need to be implemented for once. 
Consider the definition of matrix multiplication, it needs $C_N$ AND gates to apply 
bit-wise multiplication and $C_N-1$ XOR gates to sum the intermediate products up. The number of nonzero entries
in the $\lambda$-Matrix can be counted: $C_N = 27$.
As a result, the most naive NB multiplier design (or Massey-Omura multiplier \cite{MasseyOmura})
contains 53 gates in total, which is a great saving in area cost comparing to StdB multipliers.

\section{Design a Normal Basis Multiplier on Gate Level}
The NB multiplier design consumes much less gates than ordinary StdB multiplier design, even if 
we use the most naive design. However, the modern NB multiplier design has been improved a lot from the 
very first design model proposed by Massey and Omura in 1986 \cite{MasseyOmura}. In order to 
test our approach on practical contemporary circuits, it is necessary to learn the mechanism and design 
routine of several kinds of modern NB multipliers.
\subsection{Sequential Multiplier with Parallel Outputs}
The major benefit of NB multiplier origins from the sequential design. A straightforward design implementing 
the cyclic-shift of operands and $\lambda$-Matrix logic component is the Massey-Omura multiplier.

% fig:MasseyOmura

Figure \ref{fig:MasseyOmura} shows the basic architecture of a Massey-Omura multiplier. The operands
$A$ and $B$ are 2 arrays of flip-flops which allow 1-bit right-cyclic shift every clock cycle.
The logic gates in the boxes implements the matrix multiplication with $\lambda$-Matrix $M^{(0)}$, 
while each AND gate corresponds to term $a_ib_j$ and each XOR gate corresponds to addition $a_ib_j+a_{i'}b_{j'}$. 
The XOR layer has only 1 output, giving out 1 bit of product $C$ every clock cycle.

The behavior of Massey-Omura multiplier can be concluded as: pre-load operands $A,B$ and reset $C$ to 0, after 
executing for $k$ clock cycles, the data stored in flip-flop array $C$ is the product $A\times B$.
We note that there is only one output giving 1 bit of the product each clock cycle, which matches the 
definition of serial output to communication channel. Therefore this type of design is named as 
sequential multiplier with serial output (SMSO).
The SMSO architecture need $C_N$ AND gates and $C_N-1$ XOR gates, which equals to $2k-1$ AND gates and $2k-2$ XOR gates
if it is designed using ONB. In fact, the number of gates can be reduced if the multiplication is 
implemented using a conjugate of SMSO.

The gate-level logic boxes are implementing following function:
\begin{equation}
\label{eqn:SMPOterms}
c_l = row_1(A\times M^{(l)}) \times B + row_2(A\times M^{(l)}) \times B + \cdots + row_{k}(A\times M^{(l)}) \times B
\end{equation}
It can be decomposed into $k$ terms. If we only compute one term for each $c_l,~0\leq l\leq k-1$ in one 
clock cycle, make $k$ outputs and add them up using shift register after $k$ clock cycles, 
it will generate the same result with SMSO. This kind of architecture is named as 
sequential multiplier with parallel outputs (SMPO). The basic SMPO, as a conjugate of Massey-Omura multiplier,
is invented by Agnew et al. \cite{agnew1991implementation}.

\begin{figure}[H]
\centering{
\includegraphics[width=\textwidth]{newfig/mySMPO.eps}
\caption{5-bit Agnew's SMPO. Index $i$ satisfies $0<i<4$, indices $u,v$ are determined by column \# of nonzero entries in $i$-th row of $\lambda$-Matrix $M^{(0)}$, i.e. if entry $M_{ij}^{(0)}$ is a nonzero entry, $u$ or $v$ equals to $i+j \pmod 5$. Index $w = 2i\pmod 5$}
\label{fig:SMPO}}
\end{figure}

\begin{Example}[5-bit Agnew's SMPO]
Given GF $\mathbb F_{2^5}$ and primitive element $\alpha$ defined by irreducible polynomial 
$\alpha^5+\alpha^2+1=0$, normal element $\beta = \alpha^5$ constructs an ONB $\{\beta,\beta^2,\beta^4,\beta^8,\beta^{16}\}$.
The $0$-th $\lambda$-Matrix for this ONB is
\begin{equation*}
M^{(0)} = \left(\begin{array}{lcccr}
0 &1 &0 &0 &0 \\
1 &0 &0 &1 &0 \\
0 &0 &0 &1 &1 \\
0 &1 &1 &0 &0 \\
0 &0 &1 &0 &1
\end{array}\right)
\end{equation*}
Then a typical design of 5-bit Agnew's SMPO is depicted in figure \ref{fig:SMPO}.

The operands part of this circuit is the same with Massey-Omura multiplier. The differences are on 
the matrix multiplication part, while it is implemented as separate logic blocks for 5 outputs,
and the 5 blocks are connected in a shift register fashion. By analyzing the detailed function of 
logic blocks, we can reveal the mechanism of Agnew's SMPO.

Suppose we implement $M^{(0)}$ as the logic block in SMSO. In the first clock cycle, the output is
\begin{equation}
\label{eqn:5bitSMPO_c0}
c_0 = a_1b_0+(a_0+a_3)b_1 + (a_3+a_4)b_2 + (a_1+a_2)b_3 + (a_2+a_4)b_4
\end{equation}
Note it is written in the form of equation \ref{eqn:SMPOterms}. In next clock cycles we can obtain 
remaining bits of the product, which can be written in following general form polynomial:
\begin{align}
c_i =& b_ia_{i+1} + b_{i+1}(a_i + a_{i+3}) + b_{i+2}(a_{i+3} + a_{i+4}) \nonumber\\
&+ b_{i+3}(a_{i+1} + a_{i+2}) + b_{i+4}
(a_{i+2} + a_{i+4}),\ 0\leq i\leq 4 \nonumber
\end{align}
Note all index calculations are reduced modulo 5.

Now let us observe the behavior of 5-bit Agnew's SMPO. Initially all DFFs are reset to 0. 
In the first clock cycle,
signal sent to the flip-flop in block $R_0$ denotes function:
$$R_0^{(1)} = a_1b_0$$
It equals to the first term of equation \ref{eqn:5bitSMPO_c0}. In the second clock cycle,
this signal is sent to block $R_1$ through wire $r_0$, and this block also receives data 
from operands (shifted by 1 bit), generating signal $a_u,a_v$ and $b_w$. Concretely,
signal sent to flip-flop in block $R_1$ is:
$$R_1^{(2)} = R_0^{(1)} + (a_0+a_3)b_1 = a_1b_0+(a_0+a_3)b_1$$
which forms first 2 terms of equation \ref{eqn:5bitSMPO_c0}. Similarly, we track the signal 
on $R_2$ in third clock cycle, signal on $R_3$ in fourth clock cycle, finally we can get 
$$R_4^{(5)} = a_1b_0+(a_0+a_3)b_1 + (a_3+a_4)b_2 + (a_1+a_2)b_3 + (a_2+a_4)b_4$$
which equals to $c_0$ in equation \ref{eqn:5bitSMPO_c0}.
After the fifth clock cycle ends, this signal can be detected on wire $r_0$. It shows that 
the result of $c_0$ is computed after 5 clock cycles and given on $r_0$.

If we track $R_1\to R_2\to R_3 \to \cdots \to R_0$, we can obtain $c_1$ respectively.
Thus we conclude that Agnew's SMPO functions the same with Massey-Omura multiplier.
\end{Example}

The design of Agnew's SMPO guarantees that there is only one AND gate in each $R_i$ block.
For ONB, adopting Agnew's SMPO will reduce the number of AND gates from $2k-1$ to $k$.

\subsection{Multiplier not based on $\lambda$-Matrix}
Both Massey-Omura multiplier and Agnew's SMPO rely on the implementation of $\lambda$-Matrix,
which means that they will be identical if unrolled to full combinational circuits. 
After Agnew's work of parallelization, researchers proposed more designs of SMPO, 
some of them jump out of the circle and are independent from $\lambda$-Matrix.
One competitive multiplier design of this type is invented by Reyhani-Masoleh and Hasan 
\cite{RHmulti}, which is therefore called RH-SMPO.

% fig:5bitRH

Figure \ref{fig:5bitRH} is a 5-bit RH-SMPO which is functionally equivalent to 5-bit Agnew's SMPO 
in figure \ref{fig:SMPO}. A brief proof is as follows: 

\begin{Proof}
First, we define an auxiliary function for $i$-th bit 
\begin{equation}
\label{eqn:aux}
F_i(A,B) = a_ib_i\beta + \sum_{j=1}^v d_{i,j}\beta^{1+2^j}
\end{equation}
where $0\leq i\leq k-1, v = \lfloor k/2\rfloor, 1\leq j \leq v$.
The $d$-layer index $d_{i,j}$ is defined as
\begin{equation}
\label{eqn:auxDC}
d_{i,j} = c_{a,i} c_{b,i} = (a_i+a_{i+j})(b_i+b_{i+j}),~~1\leq j\leq v
\end{equation}
$i+j$ here is the result reduced modulo $k$. Note that there is a special boundary case when
$k$ is an even number ($v = \frac{k}{2}$):
$$d_{i,v} = (a_i+a_{i+v})b_i$$
With the auxiliary function, we can utilize following theorem (proof refers to \cite{RHmulti}):
\begin{Theorem}
Consider three elements $A,B$ and $R$ such that $R=A\times B$ over $\Fkk$. Then,
$$R=(((F_{k-1}^2+F_{k-2})^2+F_{k-3})^2+\cdots+F_1)^2+F_0$$
\end{Theorem}
This form is called inductive sum of squares, and corresponds to the cyclic shifting on 
$R_i$ flip-flops. Concretely, the multiplier behavior is an implementation of 
following algorithm:

\begin{algorithm}[H]
\SetAlgoNoLine

 \KwIn{$A,B\in \Fkk$ given w.r.t. NB $N$}
 \KwOut{$R=A\times B$}
%%%%%%%%%%%%%%%%%%%%
  Initialize $A,B$ and aux var $X$ to 0\;
  \For { ($i=0$;   $i < k$; ++i ) }
  {
	$X \gets X^2+F_{k-1}(A,B)$ \CommentSty{/*use aux-func from Eq.\ref{eqn:aux}*/\;}
	$A\gets A^2,~B\gets B^2$ \CommentSty{/*Right-cyclic shift $A$ and $B$*/\;}
  }
  $R\gets X$
\caption{NB Multiplication Algorithm in RH-SMPO \cite{RHmulti}}\label{alg:RHmulti}
\end{algorithm}

In this algorithm, we use a fixed auxiliary function $F_{k-1}$ inside the loop.
This is because of equation
$$F_{k-l} = F_{k-1}(A^{2^{l-1}},B^{2^{l-1}}),~~1\le l\le k$$
So using fixed $F_{k-1}$ and squaring $A^{2^i}$ every time inside the loop is equivalent to computing 
$F_{k-1},F_{k-2},\dots,F_0$ with fixed operands $A,B$.
\end{Proof}

To better understand the mechanism of RH-SMPO, we will use this 5-bit 
RH-SMPO as an example and introduce the details on how to design it.
\begin{Example}[Designing a 5-bit RH-SMPO]
From equation \ref{eqn:aux} we can deploy AND gates in $d$-layer according to $d_{i,j}$,
and XOR gates in $c$-layer according to equation \ref{eqn:auxDC}. Concretely, as algorithm \ref{alg:RHmulti}
describes, we implement auxiliary function $F_{k-1}$ in the logic:
$$i = k-1 = 4;~~v=\lfloor 5/2 \rfloor = 2$$
\begin{equation}
\label{eqn:5bitRHaux}
F_{4}(A,B) = a_4b_4\beta+\sum_{j=1}^2 d_{4,j}\beta^{1+2^j} = d_0\beta+\sum_{j=1}^2 d_{4,j}\beta^{1+2^j}
\end{equation}
Consider indices $4+1=0\bmod 5,~4+2=1\bmod 5$, write down gates in $c$-layer and $d$-layer (besides $d_0$)
$$c_1 = a_0+a_4,~c_2 = b_0+b_4,~d_1=d_{4,1}= c_1c_2 = (a_4+a_0)(b_4+b_0)$$
$$c_3 = a_1+a_4,~c_4 = b_1+b_4,~d_2=d_{4,2}= c_3c_4 = (a_4+a_1)(b_4+b_1)$$
The difficult part of the whole design is to deploy XOR gates in $e$-layer. 
As the logic layer closest to the outputs $R_i$, $e$-layer actually finishes the implementation of 
$F_{k-1}(A,B)$. But it is not a simple addition; the reason is before bit-wise adding to $X^2$, it is necessary to 
turn the sum to NB form. In other words, theoretically we need $k$ XOR gates in $e$-layer, the output of 
$i$-th gate corresponds to the bit multiplying $\beta^{2^i}$.

In order to obtain information 
indicating interconnections between $d$-layer and $e$-layer, we need to interpret $\beta^{1+2^j}$ 
to NB representation.
There is a concept called {\bf multiplication table} (M-table) which can assist this interpretation. It is defined as 
a $k\times k$ matrix $T$ over $\mathbb F_2$:
\begin{equation}
\label{eqn:multitable}
\begin{bmatrix}
\beta^{1+2^0} \\ \beta^{1+2^1} \\ \beta^{1+2^2} \\ \vdots \\ \beta^{1+2^{k-1}}
\end{bmatrix}
= \beta
\begin{bmatrix}
\beta \\ \beta^2 \\ \beta^4 \\ \vdots \\ \beta^{2^{k-1}}
\end{bmatrix}
=
\begin{bmatrix}
T_{0,0}      &   T_{0,1}        & \dots & T_{0,k-1}\\
T_{1,0}    &   T_{1,1}           & \dots & T_{1,k-1}\\
T_{2,0}    &   T_{2,1}           & \dots & T_{2,k-1}\\
\vdots & \vdots              & \ddots     & \vdots \\
T_{k-1,0}    &   T_{k-1,1}           & \dots & T_{k-1,k-1}
\end{bmatrix}
\begin{bmatrix}
\beta \\ \beta^2 \\ \beta^4 \\ \vdots \\ \beta^{2^{k-1}}
\end{bmatrix}
= {\bf T}
\begin{bmatrix}
\beta \\ \beta^2 \\ \beta^4 \\ \vdots \\ \beta^{2^{k-1}}
\end{bmatrix}
\end{equation}

It is a known fact that M-table $T$ can be converted from $\lambda$-Matrix $M$:
$$M_{i,j}^{(0)} = T_{j-i,-i}$$
with indices reduced modulo $k$. Thus we can write down the M-table of $\mathbb F_{2^5}$ with current NB $N$:

% fig:multitable

Note that we only use row 1 and row 2 from the M-table since range $1\leq j \leq 2$.
All nonzero entries in these 2 rows corresponds to the interconnections between $d$-layer and 
$e$-layer. For example, row 1 has two nonzero entries at column 0 and column 3, which corresponds to interconnections 
between $d_1$ and $e_0,e_3$. This conclusion comes from row 1 in equation \ref{eqn:multitable}:
\begin{equation*}
\beta\cdot\beta^2 = 
\begin{bmatrix}
1 & 0 & 0 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
\beta \\ \beta^2 \\ \beta^4 \\ \beta^8 \\ \beta^{16}
\end{bmatrix}
= \beta + \beta^{2^3}
\end{equation*}

Similarly, from row 2 of M-table we derive that $d_2$ has fanouts $e_3,e_4$:
\begin{equation*}
\beta\cdot\beta^{2^2} = 
\begin{bmatrix}
0 & 0 & 0 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
\beta \\ \beta^2 \\ \beta^4 \\ \beta^8 \\ \beta^{16}
\end{bmatrix}
= \beta^{2^3} + \beta^{2^4}
\end{equation*}

Let us look back at equation \ref{eqn:5bitRHaux}, we already dealt with the latter part.
The first term is always $d_0\beta$, which denotes $d_0$ should always be connected to $e_0(\beta)$.
After gathering all interconnection information, we can translate it to gate-level circuit implementation:
$$e_0 = d_0+d_1,~e_3=d_1+d_2,~e_4=d_2$$

Then the last mission is to implement the output $R_i$ layer. Assume $r_{i-1}$ is the output of 
$R_{i-1}$ in last clock cycle, we can connect using relation 
$$R_i = r_{i-1} + e_i$$
In this example, according to the M-table in figure \ref{fig:multitable}, columns $e_1,e_2$
have only zeros in its intersection with row $d_1,d_2$. Thus gates for $e_1,e_2$ can be omitted.

This finishes the full design procedure for a 5-bit RH-SMPO.
\end{Example}

The area cost of RH-SMPO is even smaller than Agnew's SMPO. XOR gates corresponds to all nonzero entries 
in M-table, which is with the same number of nonzero entries in $\lambda$-Matrix ($C_N$). The number of AND gates 
equals to $v$ plus 1 (for gate $d_0$). When using ONB ($C_N = 2k-1$), the total number of gates 
is $2k+\lfloor \frac{k}{2}\rfloor$.

\subsection{Customized Galois Field for Optimal Normal Basis Multiplier Design}
