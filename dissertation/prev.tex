\chapter{Previous Work}
\label{ch:prev}
\section{Sequential Equivalence Checking}
As an important component of formal verification for sequential circuits, SEC techniques 
have been developed over decades and widely utilized in both academia and industry. 
The specification of a sequential circuit can also be modeled as a golden state machine,
SEC is performed to compare between the circuit for test and the golden one.
The n\"aive method to implement SEC is: preload both circuits to the same initial states, 
and assign their primary inputs to the same values during all clock-cycles.
This method needs to be operated along with state space traversal,  therefore it is 
less efficient. Moreover,  most SEC only check the primary outputs/inputs consistency 
and does not require the 1:1 state correspondence,  so state space traversal is not always necessary.

Researchers proposed improvements by using Boolean functions represent 
a set of states/transitions \cite{coudert2003unified, coudert1990verification},  or by dividing the sequential circuit to 
smaller subcircuit and re-model the FSM to conditional FSMs \cite{khasidashvili2004theoretical}. IBM created a toolset with 
interfaces that focuses on only designated initial states and remove redundancies in state space \cite{baumgartner2007scalable}.

Another direction to improve SEC algorithms is to avoid using state space traversal. 
The forward retiming method \cite{van1998sequential} and time-frame merging \cite{stoffel1997record} 
all work on an array of time-frames,  with the assistance of combinational equivalence checking (CEC)
techniques. These techniques require structural similarities between the two circuits.

The most significant difference of sequential circuits from combinational circuits is 
that the outputs of the circuit depend not only 
on the primary inputs, but also on current state. 
The behavioral difference reflect on the structual design of circuits and result in 
the existance of memory components such as latches and flip-flops.
In order to test certain properties on some signals accross multiple clock-cycles,
the most straightforward method is to propogate those signals throughout 
all clock-cycles. Moreover, for formal verification, all signals on all paths from the circuit
need to be propogated through multiple clock-cycles.
This indicates a time-to-space conversion, where 
a cut is set on the latches/flip-flops and the combinational part of circuit
is copied over several time-frames then connected together.
The procedure is called {\it unrolling} of sequential circuit, as 
Figure \ref{fig:unrolling} shows.

% fig:unrolling

Unrolling provides a way to transform a sequential circuit to combinational 
circuit. Therefore,  methods which can be applied to combinational circuit 
verification is also suitable for unrolled sequential circuits. The canonical 
graphical representation of the combinational circuit after unrolling is also 
the canonical representation of the original sequential circuit. For the sequential 
equivlance checking problem, we can also unroll the circuit to be verified and 
specification to combinational ones, then perform combinational equivlance checking
techniques. In the following part we review research and techniques which 
can be applied to unrolled sequential circuits.

\subsection{Canonical Decision Diagrams}
The decision diagrams (DDs) are invented by inspirations of 
using optimized data structures to accelerate formal verification.
The most fundamental DD is the binary DD (BDD), which origins from 
Shannon's expansion:
\begin{equation}
f(x, y, \dots) = x f_x + x' f_{x'}
\end{equation}
where $f_x = f(x = 1)$ and $f_{x'} = f(x = 0)$ denote the positive and
negative co-factors of $f$ w.r.t. $x$, respectively.
A BDD is usually represented as a binary tree.
Its ordered and reduced form -- the Reduced Ordered Binary Decision Diagram (ROBBD)
\cite{BRYA86}, was the first significant contribution because of its canonicality.  
 ROBDDs represent a Boolean function as an
implicit set of points on a canonical directed acyclic graph
(DAG). Manipulation of Boolean functions can then be carried out as
composition operations on their respective DAGs. 

Following BDDs,  variants of the Shannon's decomposition principle
were explored to develop other functional decision diagrams such as
 FDDs \cite{okfdd}, ADDs \cite{add}, MTBDDs \cite{mtbdd}, and their hybrid 
edge-valued counterparts, HDDs \cite{hdd} and EVBDDs \cite{evbdd}. 
Zero-suppressed BDDs (ZDDs) \cite{minato1993zero,minato1994calculation} use the if-then-else branches
to represent the existence of variables in a cube, and result in lower 
space complexity. It can be used to represent polynomials with integer coefficients.

% fig: ROBDD vs ZDD

DDs above are all based on bit-level operations. Even in the {\it Word-Level Decision Diagrams}
\cite{WLS}, the decomposition is still point-wise, binary, 
w.r.t. each Boolean variable. These representations do not
serve the purpose of word-level abstraction from bit-level
representations. 

Binary Moment Diagrams (BMDs) \cite{bmd}, and its derivatives K*BMDs
\cite{kbmd} and *PHDDs \cite{phdd}, perform the decomposition of a {\it linear} function
based on its two moments instead of relying on Boolean decomposition. 
MODDs \cite{modd} \cite{modd_tcomp} are a DAG representation of the
characteristic function of a circuit over Galois fields $\Fkk$. 
However, MODDs fails to work large circuits because of its poor scalability.


Taylor Expansion Diagrams (TEDs) \cite{ted_tcomp} are a
word-level canonical representation of a {\it polynomial expression},
based on the Taylor's series expansion of a polynomial. However, they do
not represent a {\it polynomial function} canonically. 

The use of DDs in traditional formal verification has a lot of advantages. 
For example, DD-based model checking is very efficient as long as the DDs of sequential 
circuit can be setup. The existence of violating states in constructed DDs 
immediately deduces the violation of property. However, when the design gets
larger and larger, the time and space cost of building and storing the diagram 
increases rapidly. In our experiment of verifying a $k$-bit arithmetic circuit 
using ZDDs, when $k$ is larger than 100, the construction of ZDDs occupies 
over $99\%$ runtime of the whole procedure.
\subsection{Combinational Equivalence Checking Techniques}
The CEC problem can be solved using methods from many aspects.
Besides using canonical DDs (BDDs
\cite{BRYA86} and their word-level variants \cite{WLS}),
non-canonical representations And-Invert-Graph (AIG) based reductions \cite{AIG:2002}
\cite{alanmi:cec:iccad2006} is also very effective. 
Solvers for satisfiability problems (SAT) are good candidates to solve CEC problems,
as long as the miter circuit of two circuits can be described using conjunctive normal form (CNF)
formulas. Applications of SAT on CEC include circuit-SAT solvers \cite{csat}, etc.,
If the circuits being compared are structurally highly similar, AIG and circuit-SAT 
based approaches are proved to be efficient.
However, when the circuits are functionally equivalent but structurally very dissimilar (e.g.,
Mastrovito \cite{mastro:1989} versus Montgomery implementations
\cite{acar:1998} of Galois field circuits), none of the 
  contemporary techniques including quantifier-free bit-vector 
  (QF-BV) theory based SMT-solvers \cite{Cryptol:fmcad09}
  offers a practical solution. As shown in \cite{lv:phd}, {\it none of BDDs, SAT, SMT
  solvers, nor the  ABC tool can prove design equivalence beyond
  $16$-bit circuits.}     


Recently integer polynomial based techniques \cite{ciesielski2014function,rolf:date16} are proposed  to verify the functional 
correctness of integer arithmetic circuits. Their approach formulates the output signature as a polynomial function 
with binary variables and integer coefficients, then rewrite the polynomial by substituting gate output with gate inputs
variables. After going through the backward rewriting procedure using reverse topological variable ordering, the polynomial
will be composed by only input variables. Then the polynomial is converted to a canonical representation, and compared
with designated input signature. If they are equivalent, then the arithmetic circuit is successfully verified.
This approach has a risk of polynomial-size-explosion during the backward rewriting. The authors proposed a heuristic
to levelize arithmetic circuit, and substitute several gates' variables at the same time to minimize the risk. However, 
the heuristic is proved to be less effective when the inner symmetry of circuit structure is missing.

To conclude, automatic formal verification of large {\it
    custom-designed modulo-arithmetic circuits} largely remains
  unsolved today.
  
\section{Symbolic Model Checking and Abstraction Refinement}
Model checking is a way to verify certain safety and liveness properties 
in sequential circuits. Symbolic model checking which avoids using explicit state encodings
provides more flexibilities to reduce the state space and enhance the 
efficiency of model checkers. The implementations of symbolic model checking 
requires canonical DDs or SAT solvers \cite{burch1990sequential,burch1991representing,biere1999symbolic,biere1999symbolic}.

Abstraction is a technique to minimize the state space representation by combining states with certain 
similarities. Sometimes it can effectively lower the number of states that require analysis by orders of magnitude,
without affecting the properties we need to verify. There is a kind of model checkers utilize abstracted models 
with interpolation \cite{mcmillan2003interpolation,mcmillan:cav06}.
At first, abstraction was done manually by designers. Clarke {\it et al.} \cite{clarke2000counterexample}
proposed an BDD-based automated abstraction by removing spurious paths from analysis of counterexamples. 
Zhang {\it et al.} \cite{zhang2005design} proposed another abstraction method based on CNF-SAT.
It implemented latch abstraction by removing "irrelevant" latches from the analysis
according to SAT/UNSAT analysis from the $k$-BMC. H. Jain {\it et al.} \cite{HimanshuDAC2005} improved the abstraction refinement technique of \cite{clarke2000counterexample},
where they use CNF-SAT to perform the refinement instead of using BDDs. The new approach is applied to verify RTL Verilog
and was known to be successful.

$k$-BMC with interpolation is a purely incremental model checking approach, and the interpolation procedure relies
on UNSAT core analysis. To overcome these weaknesses, a hybrid model checker named as IC3 is developed 
\cite{bradley2011sat} \cite{bradley2011incremental}. IC3 works incrementally to find out inductive subclauses
of negations of reached states, meanwhile it is monolithic when computing over-approximations to sets of reachable
states within $1,2,\dots,k$ steps. It is proved to be more efficient than interpolation based model checking
although using similar mechanisms.

Above techniques have limitations: they are all relying on bit-level informations from 
the circuit, which prevent them from being applied to circuit with large datapaths.
Meanwhile, their implementation replies on SAT/BDDs, which is an extension of Boolean 
functions and not compatible with other forms of constraints.

\section{Word-level Techniques applied to Sequential Circuit Synthesis and Validation}
To better verify word-level designs, word-level verification techniques are 
explored in recent years. Directly translating bit-vectors problem to bit-level 
problems is called {\it bit-blasting}, and usually brings high computational complexity issues.
Attempts to develop pure word-level techniques can be found in
the rich domain of formal model checking \cite{BHEL96} \cite{SMV},
theorem proving \cite{arditi:bmd}, bit-vector SMT-solvers
\cite{boolector} \cite{cvc3} \cite{z3} \cite{bitvector98}, automated
decision procedures for Presburger arithmetic \cite{presburger}
\cite{bultan:mixed_verification}, algebraic manipulation techniques 
\cite{devadas:algebraic_manipulation_iccd91}, or the ones based on
term re-writing \cite{AST}, etc. % TODO expand term rewriting

Polynomial, integer and other non-linear representations have also
been researched: Difference Decision Diagrams (DDDs) \cite{ddd-csl99} \cite{ddd-mt-98}, interval
diagrams \cite{interval_dd}, interval analysis using polynomials
\cite{polynomial_sanchez99}, etc. Most of these have found 
application in constraint satisfaction for simulation-based
validation:  \cite{Ritter99} \cite{hsat} \cite{lpsat}
\cite{brinkmann:asp-dac} \cite{Huang:tcad01} \cite{bitvector98}. Among
these, \cite{brinkmann:asp-dac} \cite{Huang:tcad01} \cite{bitvector98}
have been used to {\it solve} integer modular arithmetic on linear
expressions -- a different application from {\it representing}
finite field modulo-arithmetic on polynomials in a canonical form.   

Uninterpreted function abstraction is also an important category of 
word-level techniques which facilitates word-level model checking.
For uninterpreted functions, researchers manage to constrain them 
using functional consistency between word variables' evaluations
\cite{UF1,UF2,UF3}.


\section{Verification using Algebraic Geometry}

Symbolic computer algebra techniques have been employed for formal
verification of circuits over $\Z_{2^k}$ and also over
Galois fields $\Fkk$. 
General verification techniques using Gr\"obner bases
\cite{Avrunin:CAV} \cite{gbverify:2007} \cite{manna:program} are propose,
but they did not address the problems of high computational complexity to
compute Gr\"obner bases.

Verification of a combinational GF arithmetic circuit $C$ against a
polynomial specification $\F$ has been previously addressed \cite{ibm:blueveri}
\cite{lv:tcad2013} \cite{pruss:dac14}. Verification problems in
\cite{ibm:blueveri} \cite{lv:tcad2013} are formulated using
Nullstellensatz and decided using the \Grobner basis algorithm.

The paper 
\cite{pruss:dac14} performs verification by deriving a canonical
word-level polynomial representation $\F$ from the circuit $C$. Their
approach views any arbitrary Boolean function (circuit) $f: \B^k
\rightarrow \B^k$ as a polynomial function $f: \Fkk \rightarrow \Fkk$,
and derives a canonical polynomial representation $\F$ over
$\Fkk$. They show that this can be achieved by computing a reduced 
\Grobner basis w.r.t. an {\it abstraction term order} derived from the
circuit. Subsequently, they propose a \underline{r}efinement of this
\underline{a}bstraction \underline{t}erm \underline{o}rder (called
RATO), that enables to compute the \Grobner basis of a smaller subset
of polynomials. The authors show that their approach can prove
correctness of up to 571-bit combinational GF multipliers. 

\section{Concluding Remarks}
From the investigation of previous work, techniques are to be researched that 
can operate the FSM traversal on word-level to verify a property excluding spurious 
faults. We propose to solve this problem in the context of word-level verification,
with data representation, abstraction and algorithm execution all carried out 
on word-level.